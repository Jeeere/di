{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import datetime\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from scipy.stats import shapiro, levene\n",
    "import statsmodels.stats.multitest as smm\n",
    "import scikit_posthocs as sp\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logcat(log_path, verbose:bool=False):\n",
    "    '''\n",
    "    Parse the log, trimming useless content from the beginning, and adding the data to dictionaries\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Parsing logcat file...\")\n",
    "\n",
    "    log = []\n",
    "    event_log = []\n",
    "\n",
    "    with open(log_path, \"r\", encoding=\"utf-16\") as logcat_file:\n",
    "        logcat_lines = logcat_file.readlines()\n",
    "\n",
    "    # Trim the file to keep only the lines from the first scene start\n",
    "    for i in range(len(logcat_lines) - 1):\n",
    "        if '[END] Scene' in logcat_lines[i] and 'Evaluation script enabled' in logcat_lines[i + 1]:\n",
    "            # Trim the file to keep only the lines from the found pair onwards\n",
    "            logcat_lines = logcat_lines[i:]\n",
    "            if verbose:\n",
    "                print(\"File trimmed successfully.\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    # Parse the logcat file\n",
    "    for line in logcat_lines:\n",
    "        parts = line.split()\n",
    "\n",
    "        try:\n",
    "            # Get unix timestamp with milliseconds\n",
    "            current_year = datetime.datetime.now().year\n",
    "            timestamp_string =  f\"{current_year}-{parts[0]} {parts[1]}\"\n",
    "            timestamp_format = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "            dt = datetime.datetime.strptime(timestamp_string, timestamp_format)\n",
    "            unix = int(dt.timestamp() * 1000)\n",
    "\n",
    "            # Get performance stats\n",
    "            if parts[5] == \"VrApi\":\n",
    "                stats = parts[7].split(\",\")\n",
    "\n",
    "                # Create a dictionary with the stats\n",
    "                entry = {\n",
    "                    \"timestamp\": unix, \n",
    "                    \"fps\": stats[0].replace(\"FPS=\", \"\").split(\"/\")[0],\n",
    "                    \"refresh_rate\": stats[0].replace(\"FPS=\", \"\").split(\"/\")[1],\n",
    "                    \"temperature1\": stats[17].replace(\"Temp=\", \"\").replace(\"C\", \"\").split(\"/\")[0],\n",
    "                    \"temperature2\": stats[17].replace(\"Temp=\", \"\").replace(\"C\", \"\").split(\"/\")[1],\n",
    "                    \"frame_time\": stats[19].replace(\"App=\", \"\").replace(\"ms\", \"\"),\n",
    "                    \"stale_frames\": stats[4].replace(\"Stale=\", \"\"),\n",
    "                    \"free_memory\": stats[15].replace(\"Free=\", \"\").replace(\"MB\", \"\"),\n",
    "                    \"total_render_time\": stats[21].replace(\"CPU&GPU=\", \"\").replace(\"ms\", \"\"),\n",
    "                    \"gpu\": stats[24].replace(\"GPU%=\", \"\"),\n",
    "                    \"cpu_average\": stats[25].replace(\"CPU%=\", \"\").split(\"(W\")[0],\n",
    "                    \"cpu_worst\": stats[25].replace(\"CPU%=\", \"\").split(\"(W\")[1].replace(\")\",\"\")\n",
    "                }\n",
    "\n",
    "                log.append(entry)\n",
    "\n",
    "            # Get events\n",
    "            elif parts[5] == \"Unity\" and parts[4] == \"I\":\n",
    "                if \"[START]\" == parts[7] or \"[END]\" == parts[7]:\n",
    "                    event = {\"timestamp\": unix, \"state\": parts[7], \"event\": \" \".join(parts[8:])}\n",
    "                    event_log.append(event)\n",
    "\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "    return log, event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_time(log, event_log, decimals: int=2, verbose:bool=False):\n",
    "    '''\n",
    "    Adjust the timestamps in the log to be relative to the first entry.\n",
    "    Filter out entries with negative timestamps.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Adjusting timestamps to be relative to the first event...\")\n",
    "\n",
    "    first_timestamp = event_log[0][\"timestamp\"]\n",
    "\n",
    "    # Adjust timestamps to be relative to the first event (scene load)\n",
    "    for entry in log:\n",
    "        entry[\"timestamp\"] = round((entry[\"timestamp\"] - first_timestamp) / 1000, decimals)\n",
    "\n",
    "    for entry in event_log:\n",
    "        entry[\"timestamp\"] = round((entry[\"timestamp\"] - first_timestamp) / 1000, decimals)\n",
    "        \n",
    "    # Filter out performance log entries with negative timestamps\n",
    "    filtered_log = [entry for entry in log if entry[\"timestamp\"] >= 0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Timestamps adjusted successfully.\")\n",
    "        print(filtered_log)\n",
    "        print(event_log)\n",
    "\n",
    "    return filtered_log, event_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both ends of the measurements need to be trimmed as they have additional performance strain from feature initializations and user actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_logs(log, event_log, verbose:bool=False):\n",
    "    '''\n",
    "    Trim 10 seconds from the start and end of the logs.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Trimming logs...\")\n",
    "\n",
    "    # Filter events that contain the substring \"test\"\n",
    "    test_events = [event for event in event_log if \"test\" in event[\"event\"]]\n",
    "\n",
    "    # Split the logs 10 seconds before the first test event and 80 seconds after the last test event\n",
    "    first_test_event = test_events[0][\"timestamp\"]\n",
    "    last_test_event = test_events[-1][\"timestamp\"]\n",
    "\n",
    "    trimmed_log = [entry for entry in log if entry[\"timestamp\"] >= first_test_event - 15 and entry[\"timestamp\"] <= last_test_event + 30]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"First test event: {first_test_event}\")\n",
    "        print(f\"Last test event: {last_test_event}\")\n",
    "\n",
    "    return trimmed_log, event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performance(log, event_log, mode: str, verbose:bool=False):\n",
    "    '''\n",
    "    Visualize frame time over time.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Visualizing performance...\")\n",
    "\n",
    "    # Draw frame time over time\n",
    "    frame_times = [float(entry[\"frame_time\"]) for entry in log]\n",
    "    timestamps = [entry[\"timestamp\"] for entry in log]\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(timestamps, frame_times)\n",
    "    \n",
    "    added_labels = set()\n",
    "    loading_start = {}\n",
    "    max_timestamp = 0\n",
    "    event_colors = {\n",
    "        \"Scene\": \"blue\",\n",
    "        \"CO2\": \"green\",\n",
    "        \"Fade\": \"orange\",\n",
    "        \"Barchart\": \"purple\",\n",
    "        \"thermostat test\": \"red\",\n",
    "        \"CO2 test\": \"green\",\n",
    "        \"barchart test\": \"purple\",\n",
    "        \"All tests\": \"blue\"\n",
    "    }\n",
    "    test_times = []\n",
    "\n",
    "    # Draw vertical lines for events\n",
    "    for event in event_log:\n",
    "        event_type = event.get(\"event\", \"default\")\n",
    "        event_state = event.get(\"state\", \"\")\n",
    "        \n",
    "        if event_type in [\"thermostat test\", \"CO2 test\", \"barchart test\", \"All tests\"]:\n",
    "            # Default to black if event type is not in event_colors\n",
    "            color = event_colors.get(event_type, \"black\")\n",
    "\n",
    "            if event_type not in added_labels:\n",
    "                plt.axvline(x=event[\"timestamp\"], color=color, linestyle='--', label=event_type, zorder=1)\n",
    "                added_labels.add(event_type)\n",
    "            else:\n",
    "                plt.axvline(x=event[\"timestamp\"], color=color, linestyle='--', zorder=1)\n",
    "                \n",
    "            if event_state == \"[START]\":\n",
    "                loading_start[event[\"event\"]] = event[\"timestamp\"]\n",
    "            elif event_state == \"[END]\":\n",
    "                try:\n",
    "                    plt.axvspan(loading_start[event[\"event\"]], event[\"timestamp\"], color=\"gray\", alpha=0.2)\n",
    "                    test_times.append((loading_start[event[\"event\"]], event[\"timestamp\"]))\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Time for {event[\"event\"]}: {event['timestamp'] - loading_start[event[\"event\"]]}\")\n",
    "                        print(f\"    Starting time: {loading_start[event[\"event\"]]}, Ending time: {event['timestamp']}\")\n",
    "                except (NameError, TypeError) as e:\n",
    "                    print(e)\n",
    "                \n",
    "            continue\n",
    "\n",
    "        # Update max_timestamp\n",
    "        if event[\"timestamp\"] > max_timestamp:\n",
    "            max_timestamp = event[\"timestamp\"]\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frame time (ms)\")\n",
    "    plt.title(\"Frame time over time in \" + mode)\n",
    "    plt.show()\n",
    "\n",
    "    return test_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_event_log(event_log: list, num_all_tests: int = 1, ignored_time: float = 0.0, verbose:bool=False):\n",
    "    '''\n",
    "    Preprocess the event log to include idle time and combine simultaneous tests into one.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Preprocessing event log...\")\n",
    "\n",
    "    tests = [\"thermostat test\", \"CO2 test\", \"barchart test\"]\n",
    "    processed_log = []\n",
    "    last_end = None\n",
    "    all_test_events = []\n",
    "    \n",
    "    # Filter event log to only include the tests\n",
    "    event_log = [event for event in event_log if event[\"event\"] in tests]\n",
    "\n",
    "    # Loop over tests before simultaneous tests\n",
    "    for event in event_log[:-6*num_all_tests]:\n",
    "        if event[\"event\"] in tests:\n",
    "\n",
    "            if len(processed_log) == 0:\n",
    "                processed_log.append({'timestamp': ignored_time, 'state': '[START]', 'event': 'Idle'})\n",
    "                processed_log.append({'timestamp': event[\"timestamp\"] - 0.01, 'state': '[END]', 'event': 'Idle'})\n",
    "\n",
    "            if event[\"state\"] == \"[END]\":\n",
    "                last_end = event[\"timestamp\"]\n",
    "            elif event[\"state\"] == \"[START]\":\n",
    "                if last_end is not None:\n",
    "                    processed_log.append({'timestamp': last_end + 0.01, 'state': '[START]', 'event': 'Idle'})\n",
    "                    processed_log.append({'timestamp': event[\"timestamp\"] - 0.01, 'state': '[END]', 'event': 'Idle'})\n",
    "            processed_log.append(event)\n",
    "    \n",
    "    try:\n",
    "        processed_log.append({'timestamp': last_end + 0.01, 'state': '[START]', 'event': 'Idle'})\n",
    "    except TypeError as e:\n",
    "        if verbose:\n",
    "            print(e)\n",
    "            print(last_end)\n",
    "    \n",
    "    # Handle simultaneous tests\n",
    "    thermostat_test_events = [event for event in event_log if event[\"event\"] == \"thermostat test\"]\n",
    "    co2_test_events = [event for event in event_log if event[\"event\"] == \"CO2 test\"]\n",
    "    barchart_test_events = [event for event in event_log if event[\"event\"] == \"barchart test\"]\n",
    "\n",
    "    for i in range(num_all_tests):\n",
    "        # Compare the times to get the earliest end time\n",
    "        earliest_end = min(thermostat_test_events[-1][\"timestamp\"], co2_test_events[-1][\"timestamp\"], barchart_test_events[-1][\"timestamp\"])\n",
    "        latest_end = max(thermostat_test_events[-1][\"timestamp\"], co2_test_events[-1][\"timestamp\"], barchart_test_events[-1][\"timestamp\"])\n",
    "\n",
    "        earliest_start = min(thermostat_test_events[-2][\"timestamp\"], co2_test_events[-2][\"timestamp\"], barchart_test_events[-2][\"timestamp\"])\n",
    "        latest_start = max(thermostat_test_events[-2][\"timestamp\"], co2_test_events[-2][\"timestamp\"], barchart_test_events[-2][\"timestamp\"])\n",
    "        \n",
    "        all_test_events.append({\"timestamp\": earliest_start - 0.01, \"state\": \"[END]\", \"event\": \"Idle\"})\n",
    "        all_test_events.append({\"timestamp\": latest_start, \"state\": \"[START]\", \"event\": \"All tests\"})\n",
    "        all_test_events.append({\"timestamp\": earliest_end, \"state\": \"[END]\", \"event\": \"All tests\"})\n",
    "        all_test_events.append({\"timestamp\": latest_end + 0.01, \"state\": \"[START]\", \"event\": \"Idle\"})\n",
    "        all_test_events.append({'timestamp': latest_end + 60, 'state': '[END]', 'event': 'Idle'})\n",
    "\n",
    "        thermostat_test_events = thermostat_test_events[:-2]\n",
    "        co2_test_events = co2_test_events[:-2]\n",
    "        barchart_test_events = barchart_test_events[:-2]\n",
    "\n",
    "    processed_log.extend(all_test_events)\n",
    "\n",
    "    return processed_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_data(log, processed_event_log, verbose:bool=False):\n",
    "    '''\n",
    "    Binning data based on the event log.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Binning data...\")\n",
    "\n",
    "    binned_data = {\n",
    "        \"Idle\": [],\n",
    "        \"thermostat test\": [],\n",
    "        \"CO2 test\": [],\n",
    "        \"barchart test\": [],\n",
    "        \"All tests\": []\n",
    "    }\n",
    "\n",
    "    for i in range(int(len(processed_event_log) / 2)):\n",
    "        start = processed_event_log[2*i]\n",
    "        end = processed_event_log[2*i + 1]\n",
    "\n",
    "        # Filter log entries between start and end\n",
    "        binned_data[start[\"event\"]].extend([entry for entry in log if start[\"timestamp\"] <= entry[\"timestamp\"] <= end[\"timestamp\"]])\n",
    "\n",
    "    # Print all key value pairs on separate lines\n",
    "    for key, value in binned_data.items():\n",
    "        if verbose:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    return binned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stale frame analysis was only for testing purposes; was not used in the final analysis of the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stale_frames(binned_data):\n",
    "    '''\n",
    "    Calculate the total number of stale frames in each bin.\n",
    "    '''\n",
    "    stale_frames = {}\n",
    "\n",
    "    for key, value in binned_data.items():\n",
    "        total_stale_frames = sum([int(entry[\"stale_frames\"]) for entry in value])\n",
    "        stale_frames[key] = total_stale_frames\n",
    "\n",
    "    return stale_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_stale_frames(binned_data, mode: str):\n",
    "    '''\n",
    "    Visualize stale frames in idle bin.\n",
    "    '''\n",
    "    idle_data = binned_data[\"Idle\"]\n",
    "    timestamps = [entry[\"timestamp\"] for entry in idle_data]\n",
    "    stale_frames = [int(entry[\"stale_frames\"]) for entry in idle_data]\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(timestamps, stale_frames)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Stale frames\")\n",
    "    plt.title(\"Stale frames in idle state in \" + mode)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix and descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_matrix(log, mode: str, ignored_keys: list = [], verbose:bool=False):\n",
    "    '''\n",
    "    Create correlation matrix from performance data.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Creating correlation matrix...\")\n",
    "        \n",
    "    # Remove ignored keys\n",
    "    for key in ignored_keys:\n",
    "        log = [{k: v for k, v in entry.items() if k != key} for entry in log]\n",
    "\n",
    "    df = pd.DataFrame(log)\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Visualize using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Correlation matrix of performance metrics in \" + mode)\n",
    "    plt.show()\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics(log, ignored_keys: list = [], decimals: int = 2, verbose:bool=False):\n",
    "    '''\n",
    "    Calculate descriptive statistics for performance data.\n",
    "    \n",
    "    Args:\n",
    "        log: List of dictionaries with performance data\n",
    "        ignored_keys: List of keys to ignore when calculating statistics\n",
    "        decimals: Number of decimals to round the statistics to\n",
    "        verbose: Print information about the process\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Calculating descriptive statistics...\")\n",
    "        \n",
    "    statistics = {}\n",
    "    \n",
    "    # Remove ignored keys\n",
    "    for key in ignored_keys:\n",
    "        log = [{k: v for k, v in entry.items() if k != key} for entry in log]\n",
    "\n",
    "    # Convert to numpy structured array\n",
    "    dtype = [(key, float) for key in log[0].keys()]\n",
    "    data = np.array([tuple(entry.values()) for entry in log], dtype=dtype)\n",
    "\n",
    "    # Calculate statistics for each metric\n",
    "    for key in data.dtype.names:\n",
    "        statistics[key] = {\n",
    "            'average': np.mean(data[key]),\n",
    "            'median': np.median(data[key]),\n",
    "            'std_dev': np.std(data[key]),\n",
    "            'min': np.min(data[key]),\n",
    "            'max': np.max(data[key])\n",
    "        }\n",
    "\n",
    "    # Round values\n",
    "    for key, value in statistics.items():\n",
    "        for metric, val in value.items():\n",
    "            statistics[key][metric] = np.round(val, decimals)\n",
    "\n",
    "    # Print as a table with original keys on the horizontal axis and nested keys on the vertical axis\n",
    "    headers = [\"Statistic\"] + list(statistics.keys())\n",
    "    table_data = []\n",
    "    \n",
    "    for stat_name in statistics[next(iter(statistics))].keys():\n",
    "        row = [stat_name] + [statistics[key][stat_name] for key in statistics.keys()]\n",
    "        table_data.append(row)\n",
    "\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    # Also print the results ready for overleaf\n",
    "    for row in table_data:\n",
    "        print(\" & \".join(map(str, row)))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of measurements\n",
    "\n",
    "- Find start and end points of events\n",
    "- Set latest start and earliest end and event time range\n",
    "- earliest start and latest end as idle range\n",
    "\n",
    "- calculate average of data\n",
    "- calculate std dev of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_measurements(performance_logs:list, event_logs:list, verbose:bool=False):\n",
    "    '''\n",
    "    Aggregate performance measurements from multiple logs.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Aggregating performance measurements...\")\n",
    "\n",
    "    # Aggregate event logs\n",
    "    preprocessed_event_logs = []\n",
    "    aggregated_event_log = []\n",
    "\n",
    "    for log in event_logs:\n",
    "        preprocessed_event_log = preprocess_event_log(log)\n",
    "        preprocessed_event_logs.append(preprocessed_event_log)\n",
    "\n",
    "    # Loop over all events\n",
    "    for i in range(len(preprocessed_event_logs[0])):\n",
    "        comparable_events = [event_log[i] for event_log in preprocessed_event_logs]\n",
    "\n",
    "        # Handle idle events\n",
    "        if all(event[\"event\"] == \"Idle\" for event in comparable_events) or all(\"test\" in event[\"event\"] for event in comparable_events):\n",
    "            if comparable_events[0][\"state\"] == \"[START]\":\n",
    "                # Append the latest start event\n",
    "                aggregated_event_log.append(max(comparable_events, key=lambda x: x[\"timestamp\"]))\n",
    "            elif comparable_events[0][\"state\"] == \"[END]\":\n",
    "                # Append the earliest end event\n",
    "                aggregated_event_log.append(min(comparable_events, key=lambda x: x[\"timestamp\"]))\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Aggregated event log and its length:\")\n",
    "        print(aggregated_event_log)\n",
    "        print(len(aggregated_event_log))\n",
    "\n",
    "        print(\"Lengths of given performance logs:\")\n",
    "        print([len(log) for log in performance_logs])\n",
    "\n",
    "    # Combine performance logs by calculating the average of each metric\n",
    "    aggregated_performance_log = []\n",
    "\n",
    "    # Loop over range of the longest log\n",
    "    for i in range(max([len(log) for log in performance_logs])):\n",
    "        # Get all entries at index i\n",
    "        entries = [log[i] for log in performance_logs if i < len(log)]\n",
    "        # Calculate the average value of each metric across all logs at each index\n",
    "        aggregated_entry = {\n",
    "            key: sum([float(entry[key]) for entry in entries]) / len(entries) for key in entries[0].keys()\n",
    "        }\n",
    "        aggregated_performance_log.append(aggregated_entry)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Aggregated performance log:\")\n",
    "        print(aggregated_performance_log)\n",
    "\n",
    "    return aggregated_performance_log, aggregated_event_log\n",
    "\n",
    "    \n",
    "\n",
    "def process_logs(log_paths:list, verbose:bool=False):\n",
    "    '''\n",
    "    Process logs from the given paths\n",
    "    '''\n",
    "    performance_logs = []\n",
    "    event_logs = []\n",
    "\n",
    "    for path in log_paths:\n",
    "        log, event_log = parse_logcat(path, verbose)\n",
    "        log, event_log = adjust_time(log, event_log, decimals=0, verbose=verbose)\n",
    "        log, event_log = trim_logs(log, event_log, verbose)\n",
    "\n",
    "        performance_logs.append(log)\n",
    "        event_logs.append(event_log)\n",
    "\n",
    "    return performance_logs, event_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_binned_data_csv(data:dict, mode:str):\n",
    "    '''\n",
    "    Combine and export binned data into a single CSV file.\n",
    "    '''\n",
    "    combined_data = []\n",
    "    for key, value in data.items():\n",
    "        for entry in value:\n",
    "            entry[\"state\"] = key.lower().replace(\" \", \"_\")\n",
    "            combined_data.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    df.to_csv(f\"./output/{mode.lower().replace(\" \", \"_\")}.csv\", index=False)\n",
    "    \n",
    "def clear_export():\n",
    "    '''\n",
    "    Clear the existing CSV file.\n",
    "    '''\n",
    "    with open(\"./output/output.csv\", \"w\") as f:\n",
    "        f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all():\n",
    "    '''\n",
    "    Export all data to a single CSV file.\n",
    "    '''\n",
    "    output_df = pd.DataFrame()\n",
    "    # Loop over files in ./input\n",
    "    for file in os.listdir(\"./input\"):\n",
    "        if file.endswith(\".log\"):\n",
    "            # Get file name\n",
    "            filename = file.replace(\".log\", \"\").replace(\"logcat_\", \"\")\n",
    "            filename_list = filename.split(\"_\")\n",
    "            \n",
    "            # Get metadata\n",
    "            mode = filename_list.pop(0).upper()\n",
    "            run = filename_list.pop(-1)\n",
    "            movement = \"liike\" in filename_list\n",
    "\n",
    "            # Process the log\n",
    "            performance_logs, event_logs = process_logs([f\"./input/{file}\"])\n",
    "            print(performance_logs)\n",
    "\n",
    "            for entry in performance_logs[0]:\n",
    "                entry['mode'] = mode\n",
    "                entry['movement'] = movement\n",
    "                entry['condition'] = mode + (\"_Movement\" if movement else \"_NoMovement\")\n",
    "                entry['run'] = run\n",
    "\n",
    "            combined_data = []\n",
    "            for datapoint in performance_logs[0]:\n",
    "                combined_data.append(datapoint)\n",
    "\n",
    "            processed_df = pd.DataFrame(combined_data)\n",
    "            # Append to output df\n",
    "            output_df = pd.concat([output_df, processed_df], ignore_index=True)\n",
    "\n",
    "    # add index column name\n",
    "    output_df.index.name = \"index\"\n",
    "    # start index from 1\n",
    "    output_df.index = output_df.index + 1\n",
    "    output_df.to_csv(f\"./output/output.csv\", mode='w', header=True, index=True)\n",
    "\n",
    "    return\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# clear_export()\n",
    "export_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_of_variance(metric:str, correction:bool):\n",
    "    '''\n",
    "    Performs analysis of variance.\n",
    "    Checks if data is normally distributed, if conditions have similar variance and if sphericity is met.\n",
    "    Performs ANOVA and Friedman test.\n",
    "    If checks fail, Friedman test should be used.\n",
    "    '''\n",
    "    print(\"-----\")\n",
    "    print(metric.upper())\n",
    "    # Load data CSV\n",
    "    data = pd.read_csv(\"./output/output.csv\", header=0)\n",
    "    # Calculate logarithm of frame time\n",
    "    data[\"log_\" + metric] = np.log(data[metric])\n",
    "\n",
    "    print(\"-----\")\n",
    "    print(\"TESTS\")\n",
    "    print(\"-----\")\n",
    "\n",
    "    # Check if data is normally distributed\n",
    "    for condition in data[\"condition\"].unique():\n",
    "        _, p = shapiro(data[data[\"condition\"] == condition][metric])\n",
    "\n",
    "        if p < 0.05:\n",
    "            print(f\"{condition} is not normally distributed\")\n",
    "        else:\n",
    "            print(f\"{condition} is normally distributed\")\n",
    "        # print(p)\n",
    "\n",
    "    # Check if conditions have similar variance\n",
    "    samples = [data[data[\"condition\"] == condition][metric] for condition in data[\"condition\"].unique()]\n",
    "    _, p = levene(*samples)\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(\"Conditions do not have similar variance\")\n",
    "    else:\n",
    "        print(\"Conditions have similar variance\")\n",
    "\n",
    "    # # Check for sphericity\n",
    "    sphericity = pg.sphericity(data, dv=metric, subject=\"run\", within=\"condition\")\n",
    "    print(\"Sphericity: \", sphericity[0])\n",
    "\n",
    "    print(\"-----\")\n",
    "    print(\"ANOVA\")\n",
    "    print(\"-----\")\n",
    "    data['mode'] = data['mode'].astype('category')\n",
    "    anova = pg.rm_anova(data=data, dv=metric, within=\"condition\", subject='run', correction=correction, detailed=True)\n",
    "\n",
    "    for row in anova.iterrows():\n",
    "        if row[1][\"p-unc\"] > 0 and row[1][\"p-unc\"] < 0.001:\n",
    "            anova.at[row[0], \"p-unc\"] = \"<0.001\"\n",
    "        elif row[1][\"p-unc\"] > 0.001 and row[1][\"p-unc\"] < 0.01:\n",
    "            anova.at[row[0], \"p-unc\"] = \"<0.01\"\n",
    "        elif row[1][\"p-unc\"] > 0.01:\n",
    "            anova.at[row[0], \"p-unc\"] = round(row[1][\"p-unc\"], 3)\n",
    "\n",
    "    # Remove unwanted columns\n",
    "    anova = anova.drop(columns=[\"p-GG-corr\", \"ng2\", \"eps\", \"sphericity\", \"W-spher\", \"p-spher\"])\n",
    "\n",
    "    # Round values\n",
    "    anova = anova.round(3)\n",
    "    \n",
    "    print(anova)\n",
    "\n",
    "    print(\"-----\")\n",
    "    print(\"T-TEST\")\n",
    "    print(\"-----\")\n",
    "\n",
    "    results = []\n",
    "    t_results = []\n",
    "\n",
    "    # Loop over all combinations of conditions\n",
    "    for c1, c2 in combinations(data[\"condition\"].unique(), 2):\n",
    "        # Get the data for the two conditions\n",
    "        x = data[data[\"condition\"] == c1][metric]\n",
    "        y = data[data[\"condition\"] == c2][metric]\n",
    "\n",
    "        # t-test\n",
    "        t_result = pg.ttest(x, y, correction=\"auto\", paired=False)\n",
    "        t_row = [c1, c2] + t_result.iloc[0].tolist()\n",
    "        t_results.append(t_row)\n",
    "\n",
    "    # Combine results into a dataframe\n",
    "    df_t = pd.DataFrame(t_results, columns=[\"Condition 1\", \"Condition 2\"] + list(t_result.columns))\n",
    "    \n",
    "    # remove alternative column from df\n",
    "    df_t = df_t.drop(columns=[\"alternative\"])\n",
    "\n",
    "    df_t[\"Condition 1\"] = df_t[\"Condition 1\"].str.replace(\"_\", \" \")\n",
    "    df_t[\"Condition 2\"] = df_t[\"Condition 2\"].str.replace(\"_\", \" \")\n",
    "\n",
    "    # Loop through p-values and format them\n",
    "    for i in range(len(df_t)):\n",
    "        if df_t.iloc[i][\"p-val\"] > 0 and df_t.iloc[i][\"p-val\"] < 0.001:\n",
    "            df_t.at[i, \"p-val\"] = \"<0.001\"\n",
    "        elif df_t.iloc[i][\"p-val\"] > 0.001 and df_t.iloc[i][\"p-val\"] < 0.01:\n",
    "            df_t.at[i, \"p-val\"] = \"<0.01\"\n",
    "        elif df_t.iloc[i][\"p-val\"] > 0.01:\n",
    "            df_t.at[i, \"p-val\"] = round(df_t.iloc[i][\"p-val\"], 3)\n",
    "\n",
    "    # Round to 2 decimals\n",
    "    df_t = df_t.round(2)\n",
    "\n",
    "    # Filter out dof and bf10 columns from df\n",
    "    df_t = df_t.drop(columns=[\"dof\", \"BF10\"])\n",
    "\n",
    "    print(tabulate(df_t, headers='keys', tablefmt='pretty'))\n",
    "    print(\"        \\\\toprule\")\n",
    "    print(r\"        Condition 1 & Condition 2 & T & p-value & CI95\\% & Cohen's d & Power \\\\\\\\\")\n",
    "    print(\"        \\\\midrule\")\n",
    "    for _, row in df_t.iterrows():\n",
    "        print(\"        \" + \" & \".join(map(str, row)) + \" \\\\\\\\\")\n",
    "    print(\"        \\\\bottomrule\")\n",
    "\n",
    "    a = pg.pairwise_tests(data, dv=metric, within=\"condition\", subject=\"run\", padjust='bonf', parametric=False, alpha=0.05, correction='auto')\n",
    "    print(a)\n",
    "\n",
    "analysis_of_variance(\"frame_time\", True)\n",
    "analysis_of_variance(\"fps\", True)\n",
    "analysis_of_variance(\"gpu\", True)\n",
    "analysis_of_variance(\"cpu_average\", True)\n",
    "analysis_of_variance(\"cpu_worst\", True)\n",
    "analysis_of_variance(\"total_render_time\", True)\n",
    "analysis_of_variance(\"stale_frames\", True)\n",
    "analysis_of_variance(\"free_memory\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_vr = [r\".\\logcat_final\\logcat_vr_1.log\", r\".\\logcat_final\\logcat_vr_2.log\", r\".\\logcat_final\\logcat_vr_3.log\", r\".\\logcat_final\\logcat_vr_4.log\", r\".\\logcat_final\\logcat_vr_5.log\"]\n",
    "paths_vr_liike = [r\".\\logcat_final\\logcat_vr_liike_1.log\", r\".\\logcat_final\\logcat_vr_liike_2.log\", r\".\\logcat_final\\logcat_vr_liike_3.log\", r\".\\logcat_final\\logcat_vr_liike_4.log\", r\".\\logcat_final\\logcat_vr_liike_5.log\"]\n",
    "paths_vr_pt = [r\".\\logcat_final\\logcat_vr-pt_1.log\", r\".\\logcat_final\\logcat_vr-pt_2.log\", r\".\\logcat_final\\logcat_vr-pt_3.log\", r\".\\logcat_final\\logcat_vr-pt_4.log\", r\".\\logcat_final\\logcat_vr-pt_5.log\"]\n",
    "paths_vr_pt_liike = [r\".\\logcat_final\\logcat_vr-pt_liike_1.log\", r\".\\logcat_final\\logcat_vr-pt_liike_2.log\", r\".\\logcat_final\\logcat_vr-pt_liike_3.log\", r\".\\logcat_final\\logcat_vr-pt_liike_4.log\", r\".\\logcat_final\\logcat_vr-pt_liike_5.log\"]\n",
    "paths_ar = [r\".\\logcat_final\\logcat_ar_1.log\", r\".\\logcat_final\\logcat_ar_2.log\", r\".\\logcat_final\\logcat_ar_3.log\", r\".\\logcat_final\\logcat_ar_4.log\", r\".\\logcat_final\\logcat_ar_5.log\"]\n",
    "paths_ar_liike = [r\".\\logcat_final\\logcat_ar_liike_1.log\", r\".\\logcat_final\\logcat_ar_liike_2.log\", r\".\\logcat_final\\logcat_ar_liike_3.log\", r\".\\logcat_final\\logcat_ar_liike_4.log\", r\".\\logcat_final\\logcat_ar_liike_5.log\"]\n",
    "\n",
    "def analyze_batch(paths:list, mode:str, verbose:bool=False):\n",
    "    '''\n",
    "    Analyze a batch of logs.\n",
    "\n",
    "    Args:\n",
    "        paths: list of paths to the log files\n",
    "        mode: string representing the conditions of the measurements\n",
    "        verbose: boolean for printing debug information\n",
    "    '''\n",
    "\n",
    "    performance_logs, event_logs = process_logs(paths, verbose)\n",
    "    performance_logs, event_logs = aggregate_measurements(performance_logs, event_logs, verbose)\n",
    "    binned_data = bin_data(performance_logs, event_logs, verbose)\n",
    "    # Loop through binned data and print the length of each bin\n",
    "    for key, value in binned_data.items():\n",
    "        print(f\"{key}: {len(value)}\")\n",
    "    export_binned_data_csv(binned_data, mode)\n",
    "\n",
    "    visualize_performance(performance_logs, event_logs, mode, verbose)\n",
    "\n",
    "    corr = create_correlation_matrix(performance_logs, mode, ignored_keys, verbose)\n",
    "    print(corr)\n",
    "\n",
    "    # stale_frames = calculate_stale_frames(binned_data)\n",
    "    # visualize_stale_frames(binned_data, mode)\n",
    "\n",
    "    print(\"Descriptive statistics for all logs:\")\n",
    "    descriptive_statistics(performance_logs, ignored_keys, decimals=2, verbose=verbose)\n",
    "\n",
    "    print(\"Descriptive statistics for idle:\")\n",
    "    descriptive_statistics(binned_data[\"Idle\"], ignored_keys, decimals=2, verbose=verbose)\n",
    "    print(\"Descriptive statistics for thermostat test:\")\n",
    "    descriptive_statistics(binned_data[\"thermostat test\"], ignored_keys, decimals=2, verbose=verbose)\n",
    "    print(\"Descriptive statistics for CO2 test:\")\n",
    "    descriptive_statistics(binned_data[\"CO2 test\"], ignored_keys, decimals=2, verbose=verbose)\n",
    "    print(\"Descriptive statistics for barchart test:\")\n",
    "    descriptive_statistics(binned_data[\"barchart test\"], ignored_keys, decimals=2, verbose=verbose)\n",
    "    print(\"Descriptive statistics for simultaneous tests:\")\n",
    "    descriptive_statistics(binned_data[\"All tests\"], ignored_keys, decimals=2, verbose=verbose)\n",
    "\n",
    "ignored_keys = [\"timestamp\", \"refresh_rate\", \"temperature1\", \"temperature2\", \"state\"]\n",
    "\n",
    "analyze_batch(paths_vr, \"VR\", verbose=False)\n",
    "analyze_batch(paths_vr_liike, \"VR with movement\", verbose=False)\n",
    "analyze_batch(paths_vr_pt, \"VR with passthrough\", verbose=False)\n",
    "analyze_batch(paths_vr_pt_liike, \"VR with passthrough and movement\", verbose=False)\n",
    "analyze_batch(paths_ar, \"AR\", verbose=False)\n",
    "analyze_batch(paths_ar_liike, \"AR with movement\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performances_together(logs:list, event_log, mode: str, verbose:bool=False):\n",
    "    '''\n",
    "    Visualize frame time over time\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Visualizing performance...\")\n",
    "\n",
    "    modes = [\"VR\", \"VR-PT\", \"AR\"]\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.ylim(bottom=0, top=25)\n",
    "    plt.yticks(np.arange(0, 26, 2.5))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for i, log in enumerate(logs):\n",
    "        frame_times = [float(entry[\"frame_time\"]) for entry in log]\n",
    "        timestamps = [entry[\"timestamp\"] for entry in log]\n",
    "        plt.plot(timestamps, frame_times, label=f\"{modes[i]}\")\n",
    "    \n",
    "    added_labels = set()\n",
    "    loading_start = {}\n",
    "    event_colors = {\n",
    "        \"Scene\": \"blue\",\n",
    "        \"CO2\": \"green\",\n",
    "        \"Fade\": \"orange\",\n",
    "        \"Barchart\": \"purple\",\n",
    "        \"thermostat test\": \"red\",\n",
    "        \"CO2 test\": \"green\",\n",
    "        \"barchart test\": \"purple\",\n",
    "        \"All tests\": \"blue\"\n",
    "    }\n",
    "    test_times = []\n",
    "\n",
    "    # Draw vertical lines for events\n",
    "    for event in event_log:\n",
    "        event_type = event.get(\"event\", \"default\")\n",
    "        event_state = event.get(\"state\", \"\")\n",
    "        \n",
    "        if event_type in [\"thermostat test\", \"CO2 test\", \"barchart test\", \"All tests\"]:\n",
    "            # Default to black if event type is not in event_colors\n",
    "            color = event_colors.get(event_type, \"black\")\n",
    "\n",
    "            if event_type not in added_labels:\n",
    "                plt.axvline(x=event[\"timestamp\"], color=color, linestyle='--', label=event_type, zorder=1)\n",
    "                added_labels.add(event_type)\n",
    "            else:\n",
    "                plt.axvline(x=event[\"timestamp\"], color=color, linestyle='--', zorder=1)\n",
    "                \n",
    "            if event_state == \"[START]\":\n",
    "                loading_start[event[\"event\"]] = event[\"timestamp\"]\n",
    "            elif event_state == \"[END]\":\n",
    "                try:\n",
    "                    plt.axvspan(loading_start[event[\"event\"]], event[\"timestamp\"], color=\"gray\", alpha=0.2)\n",
    "                    test_times.append((loading_start[event[\"event\"]], event[\"timestamp\"]))\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Time for {event[\"event\"]}: {event['timestamp'] - loading_start[event[\"event\"]]}\")\n",
    "                        print(f\"    Starting time: {loading_start[event[\"event\"]]}, Ending time: {event['timestamp']}\")\n",
    "                except (NameError, TypeError) as e:\n",
    "                    print(e)\n",
    "                \n",
    "            continue\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frame time (ms)\")\n",
    "    plt.title(\"Frame time over time including \" + mode)\n",
    "    plt.show()\n",
    "\n",
    "visualize_performances_together([performance_logs_vr, performance_logs_vr_pt, performance_logs_ar], event_logs_vr, \"all modes without movement\", verbose=False)\n",
    "visualize_performances_together([performance_logs_vr_liike, performance_logs_vr_pt_liike, performance_logs_ar_liike], event_logs_vr_liike, \"all modes with movement\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis on single emasurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_vr = r\".\\pre\\logcat_vr_liike.log\"\n",
    "pre_vr_2 = r\".\\pre\\vr-testi.log\"\n",
    "\n",
    "vr_1 = r\".\\logcat_final\\logcat_vr_1.log\"\n",
    "vr_2 = r\".\\logcat_final\\logcat_vr_2.log\"\n",
    "vr_3 = r\".\\logcat_final\\logcat_vr_3.log\"\n",
    "vr_4 = r\".\\logcat_final\\logcat_vr_4.log\"\n",
    "vr_5 = r\".\\logcat_final\\logcat_vr_5.log\"\n",
    "\n",
    "vr_liike_1 = r\".\\logcat_final\\logcat_vr_liike_1.log\"\n",
    "vr_liike_2 = r\".\\logcat_final\\logcat_vr_liike_2.log\"\n",
    "vr_liike_3 = r\".\\logcat_final\\logcat_vr_liike_3.log\"\n",
    "vr_liike_4 = r\".\\logcat_final\\logcat_vr_liike_4.log\"\n",
    "vr_liike_5 = r\".\\logcat_final\\logcat_vr_liike_5.log\"\n",
    "\n",
    "faulty_vr_liike_4 = r\".\\logcat_final\\faulty\\logcat_vr_liike_4.log\"\n",
    "\n",
    "vr_pt_1 = r\".\\logcat_final\\logcat_vr_pt_1.log\"\n",
    "vr_pt_2 = r\".\\logcat_final\\logcat_vr_pt_2.log\"\n",
    "vr_pt_3 = r\".\\logcat_final\\logcat_vr_pt_3.log\"\n",
    "vr_pt_4 = r\".\\logcat_final\\logcat_vr_pt_4.log\"\n",
    "vr_pt_5 = r\".\\logcat_final\\logcat_vr_pt_5.log\"\n",
    "\n",
    "vr_pt_liike_1 = r\".\\logcat_final\\logcat_vr_pt_liike_1.log\"\n",
    "vr_pt_liike_2 = r\".\\logcat_final\\logcat_vr_pt_liike_2.log\"\n",
    "vr_pt_liike_3 = r\".\\logcat_final\\logcat_vr_pt_liike_3.log\"\n",
    "vr_pt_liike_4 = r\".\\logcat_final\\logcat_vr_pt_liike_4.log\"\n",
    "vr_pt_liike_5 = r\".\\logcat_final\\logcat_vr_pt_liike_5.log\"\n",
    "\n",
    "ar_1 = r\".\\logcat_final\\logcat_ar_1.log\"\n",
    "ar_2 = r\".\\logcat_final\\logcat_ar_2.log\"\n",
    "ar_3 = r\".\\logcat_final\\logcat_ar_3.log\"\n",
    "ar_4 = r\".\\logcat_final\\logcat_ar_4.log\"\n",
    "ar_5 = r\".\\logcat_final\\logcat_ar_5.log\"\n",
    "\n",
    "ar_liike_1 = r\".\\logcat_final\\logcat_ar_liike_1.log\"\n",
    "ar_liike_2 = r\".\\logcat_final\\logcat_ar_liike_2.log\"\n",
    "ar_liike_3 = r\".\\logcat_final\\logcat_ar_liike_3.log\"\n",
    "ar_liike_4 = r\".\\logcat_final\\logcat_ar_liike_4.log\"\n",
    "ar_liike_5 = r\".\\logcat_final\\logcat_ar_liike_5.log\"\n",
    "\n",
    "\n",
    "test_times = []\n",
    "\n",
    "def analyze(path:str, mode:str):\n",
    "    logg, event_logg = parse_logcat(path)\n",
    "    # print_log(logg, event_logg)\n",
    "    logg, event_logg = adjust_time(logg, event_logg, decimals=0)\n",
    "    # print_log(logg, event_logg)\n",
    "    logg, event_logg = trim_logs(logg, event_logg)\n",
    "    # print_log(logg, event_logg)\n",
    "\n",
    "    # test_times.append(visualize_performance(logg, event_logg, mode))\n",
    "\n",
    "    processed_event_logg = preprocess_event_log(event_logg)\n",
    "\n",
    "    binned_data = bin_data(logg, processed_event_logg)\n",
    "\n",
    "    # stale_frames = calculate_stale_frames(binned_data)\n",
    "\n",
    "    # print(stale_frames)\n",
    "\n",
    "    visualize_stale_frames(binned_data, \"VR\")\n",
    "\n",
    "    corr = create_correlation_matrix(logg, \"VR\", ignored_keys)\n",
    "\n",
    "    descriptive_statistics(logg, ignored_keys)\n",
    "    print(logg)\n",
    "    descriptive_statistics(binned_data[\"All tests\"], ignored_keys)\n",
    "    descriptive_statistics(binned_data[\"Idle\"], ignored_keys)\n",
    "\n",
    "    return\n",
    "\n",
    "# analyze(faulty_vr_liike_4)\n",
    "# analyze(pre_vr_2, '')\n",
    "\n",
    "analyze(vr_1, 'VR with no movement')\n",
    "analyze(vr_2, 'VR with no movement')\n",
    "analyze(vr_3, 'VR with no movement')\n",
    "analyze(vr_4, 'VR with no movement')\n",
    "analyze(vr_5, 'VR with no movement')\n",
    "\n",
    "# analyze(vr_liike_1, 'VR with movement')\n",
    "# analyze(vr_liike_2, 'VR with movement')\n",
    "# analyze(vr_liike_3, 'VR with movement')\n",
    "# analyze(vr_liike_4, 'VR with movement')\n",
    "# analyze(vr_liike_5, 'VR with movement')\n",
    "\n",
    "# analyze(vr_pt_1, 'VR with no movement')\n",
    "# analyze(vr_pt_2, 'VR with no movement')\n",
    "# analyze(vr_pt_3, 'VR with no movement')\n",
    "# analyze(vr_pt_4, 'VR with no movement')\n",
    "# analyze(vr_pt_5, 'VR with no movement')\n",
    "\n",
    "# analyze(vr_pt_liike_1, 'VR with movement')\n",
    "# analyze(vr_pt_liike_2, 'VR with movement')\n",
    "# analyze(vr_pt_liike_3, 'VR with movement')\n",
    "# analyze(vr_pt_liike_4, 'VR with movement')\n",
    "# analyze(vr_pt_liike_5, 'VR with movement')\n",
    "\n",
    "# analyze(ar_1, 'AR with no movement')\n",
    "# analyze(ar_2, 'AR with no movement')\n",
    "# analyze(ar_3, 'AR with no movement')\n",
    "# analyze(ar_4, 'AR with no movement')\n",
    "# analyze(ar_5, 'AR with no movement')\n",
    "\n",
    "# analyze(ar_liike_1, 'AR with movement')\n",
    "# analyze(ar_liike_2, 'AR with movement')\n",
    "# analyze(ar_liike_3, 'AR with movement')\n",
    "# analyze(ar_liike_4, 'AR with movement')\n",
    "# analyze(ar_liike_5, 'AR with movement')\n",
    "\n",
    "# for test_time in test_times:\n",
    "#     print(test_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
